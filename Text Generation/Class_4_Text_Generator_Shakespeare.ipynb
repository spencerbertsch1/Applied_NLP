{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class 4 Text Generator-Shakespeare.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhBCvDKnVstE",
        "colab_type": "text"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67qDGHYCzj6v",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, print_function, division\n",
        "from google.colab import files\n",
        "from collections import Counter, defaultdict\n",
        "from copy import deepcopy\n",
        "from IPython.display import clear_output\n",
        "from random import randint\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "CHECKPOINT_DIR = './checkpoints/' \n",
        "TEXT_ENCODING = 'utf-8'\n",
        "\n",
        "\n",
        "\n",
        "class RNN(object):\n",
        "  \"\"\"Represents a Recurrent Neural Network using LSTM cells.\n",
        "\n",
        "  Attributes:\n",
        "    num_layers: The integer number of hidden layers in the RNN.\n",
        "    state_size: The size of the state in each LSTM cell.\n",
        "    num_classes: Number of output classes. (E.g. 256 for Extended ASCII).\n",
        "    batch_size: The number of training sequences to process per step.\n",
        "    sequence_length: The number of chars in a training sequence.\n",
        "    batch_index: Index within the dataset to start the next batch at.\n",
        "    on_gpu_sequences: Generates the training inputs for a single batch.\n",
        "    on_gpu_targets: Generates the training labels for a single batch.\n",
        "    input_symbol: Placeholder for a single label for use during inference.\n",
        "    temperature: Used when sampling outputs. A higher temperature will yield\n",
        "      more variance; a lower one will produce the most likely outputs. Value\n",
        "      should be between 0 and 1.\n",
        "    initial_state: The LSTM State Tuple to initialize the network with. This\n",
        "      will need to be set to the new_state computed by the network each cycle.\n",
        "    logits: Unnormalized probability distribution for the next predicted\n",
        "      label, for each timestep in each sequence.\n",
        "    output_labels: A [batch_size, 1] int32 tensor containing a predicted\n",
        "      label for each sequence in a batch. Only generated in infer mode.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               rnn_num_layers=1,\n",
        "               rnn_state_size=128,\n",
        "               num_classes=256,\n",
        "               rnn_batch_size=1,\n",
        "               rnn_sequence_length=1):\n",
        "    self.num_layers = rnn_num_layers\n",
        "    self.state_size = rnn_state_size\n",
        "    self.num_classes = num_classes\n",
        "    self.batch_size = rnn_batch_size\n",
        "    self.sequence_length = rnn_sequence_length\n",
        "    self.batch_shape = (self.batch_size, self.sequence_length)\n",
        "    print(\"Built LSTM: \",\n",
        "          self.num_layers ,self.state_size ,self.num_classes ,\n",
        "          self.batch_size ,self.sequence_length ,self.batch_shape)\n",
        "\n",
        "\n",
        "  def build_training_model(self, dropout_rate, data_to_load):\n",
        "    \"\"\"Sets up an RNN model for running a training job.\n",
        "\n",
        "    Args:\n",
        "      dropout_rate: The rate at which weights may be forgotten during training.\n",
        "      data_to_load: A numpy array of containing the training data, with each\n",
        "        element in data_to_load being an integer representing a label. For\n",
        "        example, for Extended ASCII, values may be 0 through 255.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: If mode is data_to_load is None.\n",
        "    \"\"\"\n",
        "    if data_to_load is None:\n",
        "      raise ValueError('To continue, you must upload training data.')\n",
        "    inputs = self._set_up_training_inputs(data_to_load)\n",
        "    self._build_rnn(inputs, dropout_rate)\n",
        "\n",
        "  def build_inference_model(self):\n",
        "    \"\"\"Sets up an RNN model for generating a sequence element by element.\n",
        "    \"\"\"\n",
        "    self.input_symbol = tf.placeholder(shape=[1, 1], dtype=tf.int32)\n",
        "    self.temperature = tf.placeholder(shape=(), dtype=tf.float32,\n",
        "                                      name='temperature')\n",
        "    self.num_options = tf.placeholder(shape=(), dtype=tf.int32,\n",
        "                                      name='num_options')\n",
        "    self._build_rnn(self.input_symbol, 0.0)\n",
        "\n",
        "    self.temperature_modified_logits = tf.squeeze(\n",
        "        self.logits, 0) / self.temperature\n",
        "\n",
        "    #for beam search\n",
        "    self.normalized_probs = tf.nn.softmax(self.logits)\n",
        "\n",
        "    self.output_labels = tf.multinomial(self.temperature_modified_logits,\n",
        "                                        self.num_options)\n",
        "\n",
        "  def _set_up_training_inputs(self, data):\n",
        "    self.batch_index = tf.placeholder(shape=(), dtype=tf.int32)\n",
        "    batch_input_length = self.batch_size * self.sequence_length\n",
        "\n",
        "    input_window = tf.slice(tf.constant(data, dtype=tf.int32),\n",
        "                            [self.batch_index],\n",
        "                            [batch_input_length + 1])\n",
        "\n",
        "    self.on_gpu_sequences = tf.reshape(\n",
        "        tf.slice(input_window, [0], [batch_input_length]), self.batch_shape)\n",
        "\n",
        "    self.on_gpu_targets = tf.reshape(\n",
        "        tf.slice(input_window, [1], [batch_input_length]), self.batch_shape)\n",
        "\n",
        "    return self.on_gpu_sequences\n",
        "\n",
        "  def _build_rnn(self, inputs, dropout_rate):\n",
        "    \"\"\"Generates an RNN model using the passed functions.\n",
        "\n",
        "    Args:\n",
        "      inputs: int32 Tensor with shape [batch_size, sequence_length] containing\n",
        "        input labels.\n",
        "      dropout_rate: A floating point value determining the chance that a weight\n",
        "        is forgotten during evaluation.\n",
        "    \"\"\"\n",
        "    # Alias some commonly used functions\n",
        "    dropout_wrapper = tf.contrib.rnn.DropoutWrapper\n",
        "    lstm_cell = tf.contrib.rnn.LSTMCell\n",
        "    multi_rnn_cell = tf.contrib.rnn.MultiRNNCell\n",
        "\n",
        "    self._cell = multi_rnn_cell(\n",
        "        [dropout_wrapper(lstm_cell(self.state_size), 1.0, 1.0 - dropout_rate)\n",
        "         for _ in range(self.num_layers)])\n",
        "\n",
        "    self.initial_state = self._cell.zero_state(self.batch_size, tf.float32)\n",
        "\n",
        "    embedding = tf.get_variable('embedding',\n",
        "                                [self.num_classes, self.state_size])\n",
        "\n",
        "    embedding_input = tf.nn.embedding_lookup(embedding, inputs)\n",
        "    output, self.new_state = tf.nn.dynamic_rnn(self._cell, embedding_input,\n",
        "                                               initial_state=self.initial_state)\n",
        "\n",
        "    self.logits = tf.contrib.layers.fully_connected(output, self.num_classes,\n",
        "                                                    activation_fn=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBtu02V2ocXC",
        "colab_type": "text"
      },
      "source": [
        "## Get the training data.\n",
        "\n",
        "We can either download the works of Shakespeare to train on or upload a plain text file (in the next section)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvoepQadkPG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shakespeare_url = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
        "import urllib\n",
        "file_contents = urllib.urlopen(shakespeare_url).read()\n",
        "file_name = \"shakespeare\"\n",
        "file_contents = file_contents[10501:]  # Skip headers and start at content\n",
        "print(\"An excerpt: \\n\", file_contents[:664])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMOW-k6sUgS9",
        "colab_type": "text"
      },
      "source": [
        "##Optional: if you want to train on your own training file, run the next cell. <p>Otherwise press 'cancel upload' to skip (or just don't run the following code cell)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzfCcEpHc7pq",
        "colab_type": "text"
      },
      "source": [
        "Any file you choose to upload must be plain text, not PDF, Microsoft Word, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7anKDCqMkrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "  if type(uploaded) is not dict: uploaded = uploaded.files  ## Deal with filedit versions\n",
        "  file_bytes = uploaded[uploaded.keys()[0]]\n",
        "  utf8_string = file_bytes.decode(TEXT_ENCODING)\n",
        "  file_contents = utf8_string if files else ''\n",
        "  file_name = uploaded.keys()[0]\n",
        "  print(\"An excerpt from training data: \\n\", file_contents[:664])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO0NDmM0VgQU",
        "colab_type": "text"
      },
      "source": [
        "# Create the LSTM \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAK1D26NKGpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(object):\n",
        "  \"\"\"Represents a Recurrent Neural Network using LSTM cells.\n",
        "\n",
        "  Attributes:\n",
        "    num_layers: The integer number of hidden layers in the RNN.\n",
        "    state_size: The size of the state in each LSTM cell.\n",
        "    num_classes: Number of output classes. (E.g. 256 for Extended ASCII).\n",
        "    batch_size: The number of training sequences to process per step.\n",
        "    sequence_length: The number of chars in a training sequence.\n",
        "    batch_index: Index within the dataset to start the next batch at.\n",
        "    on_gpu_sequences: Generates the training inputs for a single batch.\n",
        "    on_gpu_targets: Generates the training labels for a single batch.\n",
        "    input_symbol: Placeholder for a single label for use during inference.\n",
        "    temperature: Used when sampling outputs. A higher temperature will yield\n",
        "      more variance; a lower one will produce the most likely outputs. Value\n",
        "      should be between 0 and 1.\n",
        "    initial_state: The LSTM State Tuple to initialize the network with. This\n",
        "      will need to be set to the new_state computed by the network each cycle.\n",
        "    logits: Unnormalized probability distribution for the next predicted\n",
        "      label, for each timestep in each sequence.\n",
        "    output_labels: A [batch_size, 1] int32 tensor containing a predicted\n",
        "      label for each sequence in a batch. Only generated in infer mode.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               rnn_num_layers=1,\n",
        "               rnn_state_size=128,\n",
        "               num_classes=256,\n",
        "               rnn_batch_size=1,\n",
        "               rnn_sequence_length=1):\n",
        "    self.num_layers = rnn_num_layers\n",
        "    self.state_size = rnn_state_size\n",
        "    self.num_classes = num_classes\n",
        "    self.batch_size = rnn_batch_size\n",
        "    self.sequence_length = rnn_sequence_length\n",
        "    self.batch_shape = (self.batch_size, self.sequence_length)\n",
        "    print(\"Built LSTM: \",\n",
        "          self.num_layers ,self.state_size ,self.num_classes ,\n",
        "          self.batch_size ,self.sequence_length ,self.batch_shape)\n",
        "\n",
        "\n",
        "  def build_training_model(self, dropout_rate, data_to_load):\n",
        "    \"\"\"Sets up an RNN model for running a training job.\n",
        "\n",
        "    Args:\n",
        "      dropout_rate: The rate at which weights may be forgotten during training.\n",
        "      data_to_load: A numpy array of containing the training data, with each\n",
        "        element in data_to_load being an integer representing a label. For\n",
        "        example, for Extended ASCII, values may be 0 through 255.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: If mode is data_to_load is None.\n",
        "    \"\"\"\n",
        "    if data_to_load is None:\n",
        "      raise ValueError('To continue, you must upload training data.')\n",
        "    inputs = self._set_up_training_inputs(data_to_load)\n",
        "    self._build_rnn(inputs, dropout_rate)\n",
        "\n",
        "  def build_inference_model(self):\n",
        "    \"\"\"Sets up an RNN model for generating a sequence element by element.\n",
        "    \"\"\"\n",
        "    self.input_symbol = tf.placeholder(shape=[1, 1], dtype=tf.int32)\n",
        "    self.temperature = tf.placeholder(shape=(), dtype=tf.float32,\n",
        "                                      name='temperature')\n",
        "    self.num_options = tf.placeholder(shape=(), dtype=tf.int32,\n",
        "                                      name='num_options')\n",
        "    self._build_rnn(self.input_symbol, 0.0)\n",
        "\n",
        "    self.temperature_modified_logits = tf.squeeze(\n",
        "        self.logits, 0) / self.temperature\n",
        "\n",
        "    #for beam search\n",
        "    self.normalized_probs = tf.nn.softmax(self.logits)\n",
        "\n",
        "    self.output_labels = tf.multinomial(self.temperature_modified_logits,\n",
        "                                        self.num_options)\n",
        "\n",
        "  def _set_up_training_inputs(self, data):\n",
        "    self.batch_index = tf.placeholder(shape=(), dtype=tf.int32)\n",
        "    batch_input_length = self.batch_size * self.sequence_length\n",
        "\n",
        "    input_window = tf.slice(tf.constant(data, dtype=tf.int32),\n",
        "                            [self.batch_index],\n",
        "                            [batch_input_length + 1])\n",
        "\n",
        "    self.on_gpu_sequences = tf.reshape(\n",
        "        tf.slice(input_window, [0], [batch_input_length]), self.batch_shape)\n",
        "\n",
        "    self.on_gpu_targets = tf.reshape(\n",
        "        tf.slice(input_window, [1], [batch_input_length]), self.batch_shape)\n",
        "\n",
        "    return self.on_gpu_sequences\n",
        "\n",
        "  def _build_rnn(self, inputs, dropout_rate):\n",
        "    \"\"\"Generates an RNN model using the passed functions.\n",
        "\n",
        "    Args:\n",
        "      inputs: int32 Tensor with shape [batch_size, sequence_length] containing\n",
        "        input labels.\n",
        "      dropout_rate: A floating point value determining the chance that a weight\n",
        "        is forgotten during evaluation.\n",
        "    \"\"\"\n",
        "    # Alias some commonly used functions\n",
        "    dropout_wrapper = tf.contrib.rnn.DropoutWrapper\n",
        "    lstm_cell = tf.contrib.rnn.LSTMCell\n",
        "    multi_rnn_cell = tf.contrib.rnn.MultiRNNCell\n",
        "\n",
        "    self._cell = multi_rnn_cell(\n",
        "        [dropout_wrapper(lstm_cell(self.state_size), 1.0, 1.0 - dropout_rate)\n",
        "         for _ in range(self.num_layers)])\n",
        "\n",
        "    self.initial_state = self._cell.zero_state(self.batch_size, tf.float32)\n",
        "\n",
        "    embedding = tf.get_variable('embedding',\n",
        "                                [self.num_classes, self.state_size])\n",
        "\n",
        "    embedding_input = tf.nn.embedding_lookup(embedding, inputs)\n",
        "    output, self.new_state = tf.nn.dynamic_rnn(self._cell, embedding_input,\n",
        "                                               initial_state=self.initial_state)\n",
        "\n",
        "    self.logits = tf.contrib.layers.fully_connected(output, self.num_classes,\n",
        "                                                    activation_fn=None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVEdNYclTSdv",
        "colab_type": "text"
      },
      "source": [
        "### Define LSTM training parameters.\n",
        "You can leave these default values and just run this code cell as is. Later, you can come back here and experiment wth these values. \n",
        "These parameters are just for training. Further down at the inference step, we'll define parameters for the text-generation step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRPTh7_A2u80",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "num_layers = 2\n",
        "learning_rate = 0.001\n",
        "state_size = 256\n",
        "batch_size = 64\n",
        "sequence_length = 256\n",
        "num_training_steps = 30000 # Takes about 40 minuets \n",
        "steps_per_epoch = 500\n",
        "learning_rate_decay = 0.95\n",
        "gradient_clipping = 5.0\n",
        "\n",
        "def get_loss(logits, targets, target_weights):\n",
        "  with tf.name_scope('loss'):\n",
        "    return tf.contrib.seq2seq.sequence_loss(\n",
        "        logits,\n",
        "        targets,\n",
        "        target_weights,\n",
        "        average_across_timesteps=True)\n",
        "\n",
        "def get_optimizer(loss, initial_learning_rate, gradient_clipping, global_step,\n",
        "                  decay_steps, decay_rate):\n",
        "\n",
        "  with tf.name_scope('optimizer'):\n",
        "    computed_learning_rate = tf.train.exponential_decay(\n",
        "        initial_learning_rate,\n",
        "        global_step,\n",
        "        decay_steps,\n",
        "        decay_rate,\n",
        "        staircase=True)\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(computed_learning_rate)\n",
        "    trained_vars = tf.trainable_variables()\n",
        "    gradients, _ = tf.clip_by_global_norm(\n",
        "        tf.gradients(loss, trained_vars),\n",
        "        gradient_clipping)\n",
        "    training_op = optimizer.apply_gradients(\n",
        "        zip(gradients, trained_vars),\n",
        "        global_step=global_step)\n",
        "\n",
        "    return training_op, computed_learning_rate\n",
        "\n",
        "\n",
        "class LossPlotter(object):\n",
        "  def __init__(self, history_length):\n",
        "    self.global_steps = []\n",
        "    self.losses = []\n",
        "    self.averaged_loss_x = []\n",
        "    self.averaged_loss_y = []\n",
        "    self.history_length = history_length\n",
        "\n",
        "  def draw_plots(self):\n",
        "    self._update_averages(self.global_steps, self.losses,\n",
        "                          self.averaged_loss_x, self.averaged_loss_y)\n",
        "\n",
        "    plt.title('Average Loss Over Time')\n",
        "    plt.xlabel('Global Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(self.averaged_loss_x, self.averaged_loss_y, label='Loss/Time (Avg)')\n",
        "    plt.plot()\n",
        "    plt.plot(self.global_steps, self.losses,\n",
        "             label='Loss/Time (Last %d)' % self.history_length,\n",
        "             alpha=.1, color='r')\n",
        "    plt.plot()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.title('Loss for the last 100 Steps')\n",
        "    plt.xlabel('Global Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(self.global_steps, self.losses,\n",
        "             label='Loss/Time (Last %d)' % self.history_length, color='r')\n",
        "    plt.plot()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # The notebook will be slowed down at the end of training if we plot the\n",
        "    # entire history of raw data. Plot only the last 100 steps of raw data,\n",
        "    # and the average of each 100 batches. Don't keep unused data.\n",
        "    self.global_steps = []\n",
        "    self.losses = []\n",
        "    self.learning_rates = []\n",
        "\n",
        "  def log_step(self, global_step, loss):\n",
        "    self.global_steps.append(global_step)\n",
        "    self.losses.append(loss)\n",
        "\n",
        "  def _update_averages(self, x_list, y_list,\n",
        "                       averaged_data_x, averaged_data_y):\n",
        "    averaged_data_x.append(x_list[-1])\n",
        "    averaged_data_y.append(sum(y_list) / self.history_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPwFDEiZhr6x",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the LSTM\n",
        "\n",
        "\n",
        "\n",
        "Convert the plain text file into arrays of tokens (and, later,  back).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n98dKVTzkmpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "class TokenMapper(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.token_mapping = {}\n",
        "    self.reverse_token_mapping = {}\n",
        "    \n",
        "  def buildFromData(self, utf8_string, limit=0.00004):\n",
        "    print(\"Build token dictionary.\")\n",
        "    total_num = len(utf8_string)\n",
        "    sorted_tokens = sorted(Counter(utf8_string.decode('utf8')).items(), \n",
        "                           key=lambda x: -x[1])\n",
        "    # Filter tokens: Only allow printable characters (not control chars) and\n",
        "    # limit to ones that are resonably common, i.e. skip strange esoteric \n",
        "    # characters in order to reduce the dictionary size.\n",
        "    filtered_tokens = filter(lambda t: t[0] in string.printable or \n",
        "                             float(t[1])/total_num > limit, sorted_tokens)\n",
        "    tokens, counts = zip(*filtered_tokens)\n",
        "    self.token_mapping = dict(zip(tokens, range(len(tokens))))\n",
        "    for c in string.printable:\n",
        "      if c not in self.token_mapping:\n",
        "        print(\"Skipped token for: \", c)\n",
        "    self.reverse_token_mapping = {\n",
        "        val: key for key, val in self.token_mapping.items()}\n",
        "    print(\"Created dictionary: %d tokens\"%len(self.token_mapping))\n",
        "  \n",
        "  def mapchar(self, char):\n",
        "    if char in self.token_mapping:\n",
        "      return self.token_mapping[char]\n",
        "    else:\n",
        "      return self.token_mapping[' ']\n",
        "  \n",
        "  def mapstring(self, utf8_string):\n",
        "    return [self.mapchar(c) for c in utf8_string]\n",
        "  \n",
        "  def maptoken(self, token):\n",
        "    return self.reverse_token_mapping[token]\n",
        "  \n",
        "  def maptokens(self, int_array):\n",
        "    return ''.join([self.reverse_token_mapping[c] for c in int_array])\n",
        "  \n",
        "  def size(self):\n",
        "    return len(self.token_mapping)\n",
        "  \n",
        "\n",
        "  def print(self):\n",
        "    for k,v in sorted(self.token_mapping.items(),key=itemgetter(1)):\n",
        "      print(k, v)\n",
        "      \n",
        "  def alphabet(self):\n",
        "    return ''.join([k for k,v in sorted(self.token_mapping.items(),key=itemgetter(1) )])\n",
        "\n",
        "\n",
        "  \n",
        "  def save(self, path):\n",
        "    with open(path, 'wb') as json_file:\n",
        "      json.dump(self.token_mapping, json_file)\n",
        "  \n",
        "  def restore(self, path):\n",
        "    with open(path, 'r') as json_file:\n",
        "      self.token_mapping = {}\n",
        "      self.token_mapping.update(json.load(json_file))\n",
        "      self.reverse_token_mapping = {val: key for key, val in self.token_mapping.items()}\n",
        "      \n",
        "      \n",
        "# Clean the checkpoint directory and make a fresh one\n",
        "!rm -rf {CHECKPOINT_DIR}\n",
        "!mkdir {CHECKPOINT_DIR}\n",
        "!ls -lt\n",
        "\n",
        "chars_in_batch = (sequence_length * batch_size)\n",
        "file_len = len(file_contents)\n",
        "unique_sequential_batches = file_len // chars_in_batch\n",
        "\n",
        "mapper = TokenMapper()\n",
        "mapper.buildFromData(file_contents)\n",
        "mapper.save(''.join([CHECKPOINT_DIR, 'token_mapping.json']))\n",
        "\n",
        "input_values = mapper.mapstring(file_contents)\n",
        "print('Done.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5He_ECNJc61",
        "colab_type": "text"
      },
      "source": [
        "##Build the LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgXvABhpJa1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "print('Constructing model...')\n",
        "\n",
        "model = RNN(\n",
        "    rnn_num_layers=num_layers,\n",
        "    rnn_state_size=state_size,\n",
        "    num_classes=mapper.size(),\n",
        "    rnn_batch_size=batch_size,\n",
        "    rnn_sequence_length=sequence_length)\n",
        "\n",
        "model.build_training_model(0.05, np.asarray(input_values))\n",
        "print('Constructed model successfully.')\n",
        "\n",
        "print('Setting up training session...')\n",
        "neutral_target_weights = tf.constant(\n",
        "    np.ones(model.batch_shape),\n",
        "    tf.float32\n",
        ")\n",
        "loss = get_loss(model.logits, model.on_gpu_targets, neutral_target_weights)\n",
        "global_step = tf.get_variable('global_step', shape=(), trainable=False,\n",
        "                              dtype=tf.int32)\n",
        "training_step, computed_learning_rate = get_optimizer(\n",
        "    loss,\n",
        "    learning_rate,\n",
        "    gradient_clipping,\n",
        "    global_step,\n",
        "    steps_per_epoch,\n",
        "    learning_rate_decay\n",
        ")\n",
        "\n",
        "\n",
        "# Create a supervisor that will checkpoint the model in the CHECKPOINT_DIR\n",
        "sv = tf.train.Supervisor(\n",
        "    logdir=CHECKPOINT_DIR,\n",
        "    global_step=global_step,\n",
        "    save_model_secs=30)\n",
        "print()\n",
        "print('Ready to train.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tfXYyumK5z6",
        "colab_type": "text"
      },
      "source": [
        "Begin / resume the training cycle. \n",
        "This could take a while. Every so often it saves a checkpoint so we don't lose our progress. To monitor the progress of training, feel free to stop the training every so often and run the inference cell to generate text with your model.\n",
        "<p>\n",
        "This code cell will attempt to pick up training where it left off, if a previous checkpoint exists.<p>\n",
        "  This cell can be stopped if you like so you can generate text with the LSTM in the next section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wfsDMuLLUr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "start_time = datetime.now()\n",
        "\n",
        "with sv.managed_session(config=config) as sess:\n",
        "  print('Training supervisor successfully initialized all variables.')\n",
        "  if not file_len:\n",
        "    raise ValueError('To continue, you must upload training data.')\n",
        "  elif file_len < chars_in_batch:\n",
        "    raise ValueError('To continue, you must upload a larger set of data.')\n",
        "\n",
        "  plotter = LossPlotter(100)\n",
        "  step_number = sess.run(global_step)\n",
        "  zero_state = sess.run([model.initial_state])\n",
        "  max_batch_index = (unique_sequential_batches - 1) * chars_in_batch\n",
        "  while not sv.should_stop() and step_number < num_training_steps:\n",
        "    feed_dict = {\n",
        "        model.batch_index: randint(0, max_batch_index),\n",
        "        model.initial_state: zero_state\n",
        "        }\n",
        "    [_, _, training_loss, step_number, current_learning_rate, _] = sess.run(\n",
        "        [model.on_gpu_sequences,\n",
        "         model.on_gpu_targets,\n",
        "         loss,\n",
        "         global_step,\n",
        "         computed_learning_rate,\n",
        "         training_step],\n",
        "        feed_dict)\n",
        "    plotter.log_step(step_number, training_loss)\n",
        "    if step_number % 100 == 0:\n",
        "#       clear_output(True)\n",
        "      plotter.draw_plots()\n",
        "      print('Latest checkpoint is: %s' %\n",
        "            tf.train.latest_checkpoint(CHECKPOINT_DIR))\n",
        "      print('Learning Rate is: %f' %\n",
        "            current_learning_rate)\n",
        "\n",
        "    if step_number % 10 == 0:\n",
        "      print('global step %d, loss=%f' % (step_number, training_loss))\n",
        "\n",
        "# clear_output(True)\n",
        "\n",
        "print('Training completed in HH:MM:SS = ', datetime.now()-start_time)\n",
        "print('Latest checkpoint is: %s' %\n",
        "      tf.train.latest_checkpoint(CHECKPOINT_DIR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpr5zK1UUlEd",
        "colab_type": "text"
      },
      "source": [
        "# Generate text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmyNZawkJ4Qt",
        "colab_type": "text"
      },
      "source": [
        "Run the code cell below to have the LSTM automatically generate text.<p> Keep in mind the LSTM has no knowledge of text (or anything else) beyond the relatively small amount of text it was trained on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g1niaTyLtLp",
        "colab_type": "text"
      },
      "source": [
        "Try experimenting with different values for \n",
        "\n",
        "*   start_of_generated_text\n",
        "*   length_of_generated_text\n",
        "*   creativity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc8W1I0bJy-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_of_generated_text = \" Alas\"\n",
        "length_of_generated_text = 2000\n",
        "creativity = 0.85  # between 0.0 and 1.0\n",
        "\n",
        "\n",
        "class BeamSearchCandidate(object):\n",
        "  \"\"\"Represents a node within the search space during Beam Search.\n",
        "\n",
        "  Attributes:\n",
        "    state: The resulting RNN state after the given sequence has been generated.\n",
        "    sequence: The sequence of selections leading to this node.\n",
        "    probability: The probability of the sequence occurring, computed as the sum\n",
        "      of the probabilty of each character in the sequence at its respective\n",
        "      step.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, init_state, sequence, probability):\n",
        "    self.state = init_state\n",
        "    self.sequence = sequence\n",
        "    self.probability = probability\n",
        "\n",
        "  def search_from(self, tf_sess, rnn_model, temperature, num_options):\n",
        "    \"\"\"Expands the num_options most likely next elements in the sequence.\n",
        "\n",
        "    Args:\n",
        "      tf_sess: The Tensorflow session containing the rnn_model.\n",
        "      rnn_model: The RNN to use to generate the next element in the sequence.\n",
        "      temperature: Modifies the probabilities of each character, placing\n",
        "        more emphasis on higher probabilities as the value approaches 0.\n",
        "      num_options: How many potential next options to expand from this one.\n",
        "\n",
        "    Returns: A list of BeamSearchCandidate objects descended from this node.\n",
        "    \"\"\"\n",
        "    expanded_set = []\n",
        "    feed = {rnn_model.input_symbol: np.array([[self.sequence[-1]]]),\n",
        "            rnn_model.initial_state: self.state,\n",
        "            rnn_model.temperature: temperature,\n",
        "            rnn_model.num_options: num_options}\n",
        "    [predictions, probabilities, new_state] = tf_sess.run(\n",
        "        [rnn_model.output_labels,\n",
        "         rnn_model.normalized_probs,\n",
        "         rnn_model.new_state], feed)\n",
        "    # Get the indices of the num_beams next picks\n",
        "    picks = [predictions[0][x] for x in range(len(predictions[0]))]\n",
        "    for new_char in picks:\n",
        "      new_seq = deepcopy(self.sequence)\n",
        "      new_seq.append(new_char)\n",
        "      expanded_set.append(\n",
        "          BeamSearchCandidate(new_state, new_seq,\n",
        "                              probabilities[0][0][new_char] + self.probability))\n",
        "    return expanded_set\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    return self.sequence == other.sequence\n",
        "\n",
        "  def __ne__(self, other):\n",
        "    return not self.__eq__(other)\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash(self.sequence())\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "def beam_search_generate_sequence(tf_sess, rnn_model, primer, temperature=0.85,\n",
        "                                  termination_condition=None, num_beams=5):\n",
        "  \"\"\"Implements a sequence generator using Beam Search.\n",
        "\n",
        "  Args:\n",
        "    tf_sess: The Tensorflow session containing the rnn_model.\n",
        "    rnn_model: The RNN to use to generate the next element in the sequence.\n",
        "    temperature: Controls how 'Creative' the generated sequence is. Values\n",
        "      close to 0 tend to generate the most likely sequence, while values\n",
        "      closer to 1 generate more original sequences. Acceptable values are\n",
        "      within (0, 1].\n",
        "    termination_condition: A function taking one parameter, a list of\n",
        "      integers, that returns True when a condition is met that signals to the\n",
        "      RNN to return what it has generated so far.\n",
        "    num_beams: The number of possible sequences to keep at each step of the\n",
        "      generation process.\n",
        "\n",
        "  Returns: A list of at most num_beams BeamSearchCandidate objects.\n",
        "  \"\"\"\n",
        "  candidates = []\n",
        "\n",
        "  rnn_current_state = sess.run([rnn_model.initial_state])\n",
        "  #Initialize the state for the primer\n",
        "  for primer_val in primer[:-1]:\n",
        "    feed = {rnn_model.input_symbol: np.array([[primer_val]]),\n",
        "            rnn_model.initial_state: rnn_current_state\n",
        "           }\n",
        "    [rnn_current_state] = tf_sess.run([rnn_model.new_state], feed)\n",
        "\n",
        "  candidates.append(BeamSearchCandidate(rnn_current_state, primer, num_beams))\n",
        "\n",
        "  while True not in [termination_condition(x.sequence) for x in candidates]:\n",
        "    new_candidates = []\n",
        "    for candidate in candidates:\n",
        "      expanded_candidates = candidate.search_from(\n",
        "          tf_sess, rnn_model, temperature, num_beams)\n",
        "      for new in expanded_candidates:\n",
        "        if new not in new_candidates:\n",
        "          #do not reevaluate duplicates\n",
        "          new_candidates.append(new)\n",
        "    candidates = sorted(new_candidates,\n",
        "                        key=lambda x: x.probability, reverse=True)[:num_beams]\n",
        "\n",
        "  return [c for c in candidates if termination_condition(c.sequence)]\n",
        "\n",
        "\n",
        "tf.reset_default_graph()\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.InteractiveSession(config=config)\n",
        "\n",
        "model = RNN(\n",
        "    rnn_num_layers=num_layers,\n",
        "    rnn_state_size=state_size,\n",
        "    num_classes=mapper.size(),\n",
        "    rnn_batch_size=1,\n",
        "    rnn_sequence_length=1)\n",
        "\n",
        "model.build_inference_model()\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver(tf.global_variables())\n",
        "ckpt = tf.train.latest_checkpoint(CHECKPOINT_DIR)\n",
        "saver.restore(sess, ckpt)\n",
        "\n",
        "def gen(start_with, pred, creativity):\n",
        "  int_array = mapper.mapstring(start_with)\n",
        "  candidates = beam_search_generate_sequence(\n",
        "      sess, model, int_array, temperature=creativity,\n",
        "      termination_condition=pred,\n",
        "      num_beams=1)\n",
        "  gentext = mapper.maptokens(candidates[0].sequence)\n",
        "  return gentext\n",
        "\n",
        "def lengthlimit(n):\n",
        "  return lambda text: len(text)>n\n",
        "def sentences(n):\n",
        "  return lambda text: mapper.maptokens(text).count(\".\")>=n\n",
        "def paragraph():\n",
        "  return lambda text: mapper.maptokens(text).count(\"\\n\")>0\n",
        "\n",
        "\n",
        "print(gen(start_of_generated_text, lengthlimit(length_of_generated_text), creativity))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klTwA-RrFwR0",
        "colab_type": "text"
      },
      "source": [
        "##Save a copy of our trained RNN for later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdxjJaayFuhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model_to_drive = True  ## Set this to true to save directly to Google Drive.\n",
        "\n",
        "def save_model_hyperparameters(path):\n",
        "  with open(path, 'w')  as json_file:\n",
        "    model_params = {\n",
        "        'num_layers': model.num_layers,\n",
        "        'state_size': model.state_size,\n",
        "        'num_classes': model.num_classes\n",
        "    }\n",
        "    json.dump(model_params, json_file)\n",
        "\n",
        "def save_to_drive(title, content):\n",
        "  # Install the PyDrive wrapper & import libraries.\n",
        "  !pip install -U -q PyDrive\n",
        "  from pydrive.auth import GoogleAuth\n",
        "  from pydrive.drive import GoogleDrive\n",
        "  from google.colab import auth\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "\n",
        "  # Authenticate and create the PyDrive client.\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "  newfile = drive.CreateFile({'title': title})\n",
        "  newfile.SetContentFile(content)\n",
        "  newfile.Upload()\n",
        "  print('Uploaded file with ID %s as %s'% (newfile.get('id'),\n",
        "         archive_name))\n",
        "    \n",
        "archive_name = ''.join([file_name,'_seedbank_char-rnn.zip'])\n",
        "latest_model = tf.train.latest_checkpoint(CHECKPOINT_DIR).split('/')[2]\n",
        "checkpoints_archive_path = ''.join(['./exports/',archive_name])\n",
        "if not latest_model:\n",
        "  raise ValueError('You must train a model before you can export one.')\n",
        "  \n",
        "%system mkdir exports\n",
        "%rm -f {checkpoints_archive_path}\n",
        "mapper.save(''.join([CHECKPOINT_DIR, 'token_mapping.json']))\n",
        "save_model_hyperparameters(''.join([CHECKPOINT_DIR, 'model_attributes.json']))\n",
        "%system zip '{checkpoints_archive_path}' -@ '{CHECKPOINT_DIR}checkpoint' \\\n",
        "            '{CHECKPOINT_DIR}token_mapping.json' \\\n",
        "            '{CHECKPOINT_DIR}model_attributes.json' \\\n",
        "            '{CHECKPOINT_DIR}{latest_model}.'*\n",
        "\n",
        "if save_model_to_drive:\n",
        "  save_to_drive(archive_name, checkpoints_archive_path)\n",
        "else:\n",
        "  files.download(checkpoints_archive_path)\n",
        "\n",
        "  \n",
        "print('Saving complete')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}